# ACT-MetaWorld Robot Manipulation

Implementation of Action Chunking Transformer (ACT) for MetaWorld MT-1 robotic manipulation tasks, specifically the shelf-place-v3 task.

## ðŸ¤– Models Implemented

- **StandardACT**: Baseline ACT with image processing in decoder only
- **ModifiedACT**: Enhanced ACT with image-conditioned encoder

## ðŸ“ Project Structure

```
ACT-modification/
â”œâ”€â”€ configs/              # Model and training configurations
â”‚   â”œâ”€â”€ standard_act.yaml
â”‚   â””â”€â”€ modified_act.yaml
â”œâ”€â”€ models/              # Model architectures
â”‚   â”œâ”€â”€ standard_act.py
â”‚   â””â”€â”€ modified_act.py
â”œâ”€â”€ envs/                # Environment wrappers
â”‚   â”œâ”€â”€ metaworld_wrapper.py
â”‚   â””â”€â”€ metaworld_simple_wrapper.py
â”œâ”€â”€ training/            # Training code
â”‚   â”œâ”€â”€ trainer.py
â”‚   â””â”€â”€ dataset.py
â”œâ”€â”€ evaluation/          # Evaluation utilities
â”‚   â””â”€â”€ evaluator.py
â”œâ”€â”€ scripts/             # Executable scripts
â”‚   â”œâ”€â”€ train.py
â”‚   â”œâ”€â”€ collect_mt1_demos.py
â”‚   â”œâ”€â”€ evaluate_and_compare.py
â”‚   â””â”€â”€ generate_videos.py
â”œâ”€â”€ tests/               # Test files
â”œâ”€â”€ final_evaluation/    # Latest evaluation results
â”œâ”€â”€ requirements.txt     # Python dependencies
â””â”€â”€ README.md

Note: experiments/, demonstrations/, and checkpoints/ are excluded from git (too large)
```

## ðŸš€ Quick Start

### Installation

```bash
# Create conda environment
conda create -n grasp python=3.10
conda activate grasp

# Install dependencies
pip install -r requirements.txt
```

### Training

```bash
# Train StandardACT
python scripts/train.py --config configs/standard_act.yaml

# Train ModifiedACT
python scripts/train.py --config configs/modified_act.yaml
```

### Evaluation

```bash
# Compare both models
python scripts/evaluate_and_compare.py \
    --standard_checkpoint experiments/standard_act_YYYYMMDD_HHMMSS/checkpoints/best.pth \
    --modified_checkpoint experiments/modified_act_YYYYMMDD_HHMMSS/checkpoints/best.pth \
    --num_episodes 50 \
    --output_dir evaluation_results
```

## ï¿½ï¿½ Current Status

### Training Completed
- **StandardACT**: 18.74M parameters, trained for 50 epochs
  - Training loss: 0.0089
  - Validation loss: 0.0151
  
- **ModifiedACT**: 30.05M parameters, trained for 50 epochs  
  - Training loss: 0.0089
  - Validation loss: 0.0136

### Known Issues

âš ï¸ **Models currently achieve 0% success rate on evaluation**

Root causes identified:
1. **Dataset Quality**: Only 56.7% success rate in demonstrations (17/30 successful)
2. **Missing State Information**: Only 4 joint dimensions captured instead of full 39-dim state
3. **Insufficient Data**: 30 demonstrations insufficient for complex manipulation task
4. **All Demos Timeout**: All episodes run for 500 steps (no early termination)

See evaluation results in `final_evaluation/` for detailed metrics.

## ðŸ“ Key Findings

### What Works
- âœ… Model architectures implemented correctly
- âœ… Training converges (low loss)
- âœ… Evaluation pipeline functional
- âœ… No hardcoded results (reads from environment)

### What Needs Improvement
- âŒ Demonstration collection (need 50-100 high-quality demos)
- âŒ State extraction (need full 39-dim state)
- âŒ Success rate in demos (need >95% successful demonstrations)
- âŒ Data diversity (need varied trajectory lengths)

## ðŸ”§ Next Steps

1. **Fix data collection**:
   - Extract full 39-dimensional state (not just 4)
   - Collect 50-100 successful demonstrations (>95% success rate)
   - Allow early termination for successful episodes

2. **Retrain models** with improved dataset

3. **Re-evaluate** and validate performance

## ðŸ“š Dependencies

Key packages:
- PyTorch 2.0+
- MetaWorld
- Gymnasium
- NumPy
- h5py (for dataset storage)
- imageio (for video generation)

See `requirements.txt` for complete list.

## ï¿½ï¿½ Task Details

**Environment**: MetaWorld shelf-place-v3
- **Objective**: Pick up object from table and place on shelf
- **Success Criterion**: Object within 0.05-0.07m of goal position
- **Episode Length**: 500 steps max
- **Action Space**: 4D continuous (gripper x, y, z, open/close)

## ðŸ“§ Contact

For questions about this implementation, please refer to the code comments and configuration files.
